{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfmykYPRZuWz"
   },
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rZdRJYhFZTuF",
    "outputId": "05376aae-2b17-4654-c5ce-091bea287291"
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hx6FgJqZy5q"
   },
   "source": [
    "**Load EuroSAT Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "8dd2eca512f7436f8e3220e6b078c3fb",
      "e3f4b63c61d64637a164c6907dec7b54",
      "a4d48f75cf31465b863f3dc882f0f649",
      "8aaaf7a987a842cabd9df7512230f344",
      "38e1672e31af4ffca79295f08e1b1e9c",
      "d4befe7dcdb84fa9a944e1a7c2275ce3",
      "28a0a40ce07f4bb48033070364f877cd",
      "38889e67dd574b70ae6b8a38832879fd",
      "f9760f1ad72a45b68f2541110c61e5d6",
      "b8e6871d22e94b79b30a81b2c726e9e8",
      "5bfad4d67e264ee7b3c9e36016e1cd1a",
      "8805fade1c0749fa834c56ee6f9bd3c7",
      "9a4eec5e331a4d2aa81c9d95c190b80c",
      "562396d9db7049f5b9b107eb93022e6f",
      "847e7e8f36344be0839f77db4c189327",
      "df3c0a8cc3374b23ad90ba83bdeba4cf",
      "aaded87a07f44716aa50f2653164a730",
      "19b6b72ec3934104bb3860fd99e83e66",
      "fc5c3772c32c44cc8298ffa4e753a7cc",
      "61de7341a18b46529b29ab25a62f920d",
      "b65f4795557f4f5bba6282d20c9b3e23",
      "b5cf241e66bf4b5ab4e61d5c8fa28c0a",
      "5c4c030a66154509bf4d575a829b517b",
      "d733ffc73f704f2ba44c9d30ba65a425",
      "d6655a2e784744c4861106ad4d012d83",
      "9d98c67ec4d741af864e41541d8114f6",
      "5ac042314f1a47669713bb4f154665e1",
      "2db784a36af74bc0865652f239c53a93",
      "147c1adf2b344dc4b8617026a7881c39",
      "a8d02151702b4bcd9e7f5a6e0dbe9e92",
      "3565a37a91954937b1a3c4053ba72164",
      "01d0815de49740a9893d3571b2570317",
      "eca6b5b875de469a9309d8ab09043830",
      "3077227f06b64dc18a0cb4860d9a1e67",
      "8eceed37bfdc487ab6a29860de29e9fd",
      "4bddfe74b2cb457cafd756ecb2c4a365",
      "d44a2059bc1541fd8c8f36d1616e91da",
      "badb788826b844ab9f34776971957fa2",
      "df234c6b6d694da4a4be2a639f7bf8fb",
      "a91bb7ca896b4539af89e9fdf9cd7065",
      "7d881a421bcd4ffdbf143bf5bf1b84c6",
      "dca46ea4fe5849e1b69935935ac99cd3",
      "62add770512c4bbfb1dc55bbe8e7cfd8",
      "a46f051d74384b2db7ae6df7d8e17f5d",
      "b288d8fbf62e44cbb9d15b869d074669",
      "f691feebb6b247d9938a39a81717da8f",
      "5b3bbdaac45d4ac68966ad3fe36063f1",
      "9e40fb2067234dd1bd2ae347b9cd9ea3",
      "90be43044a204fd2a919bdcf04abd85d",
      "6b527fb7ced2411ca5be826d16728d17",
      "dfdfb00872cc4728a8ec4ec1683f867e",
      "01fce233c0044e7b8b4a48f20bec5661",
      "e4dd85a99616467a967385195fa4a231",
      "88144aaf908247deac3bdf6409379c3e",
      "602f72697b014fe78d495a883248fd05",
      "8eb0234c707940d2a6f554294d00466d",
      "8694b3c45ae545d5a8458392cfe75ea5",
      "cddb399638664119aca63c03ec872ace",
      "8b9cbfe7e2c240f7b4c5cd3354cc94c1",
      "99a69a4bd718483f92267bc22cc0338f",
      "988f21ea559d46e9b74c494b5fb62dec",
      "8becd841ce984ed38745080e7d7656d6",
      "104022c4f48a4ab6bf12de6c8f8f8e6a",
      "f968839795384b09a6c12031ee0c7f78",
      "ee25d76f5baa4418af07af55c9582cd3",
      "195f5dccd6dc427e84a4c3fcdf42c31e",
      "96b96aa825f74562ba04a9b6e0e6b464",
      "8ee044b8f6f14191b23a3a327cf2a69d",
      "86db00d63dbe4cfd8e3f5513c2d4116b",
      "9bde4500525f4409a4ec31273e0ddf71",
      "71c9390fb6f647469fa0440deaf72b8c",
      "e241a94863d84c75a2321ddfe079edbc",
      "e4fb72357d094ff4b0f6569d2437669a",
      "3183d7a7e7bc4b0cab6a85918965722e",
      "087c27f2e2b64e8b839c3150864961e8",
      "e2386e468ae0421ba163a094b7b88072",
      "31f03af09df74b2a8b651ffb9ca83cbe"
     ]
    },
    "id": "uKQkDmvaa76E",
    "outputId": "db2d9d08-a9fc-47d8-9a07-f59264ed308c"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"blanchon/EuroSAT_RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4ralAFudVCa"
   },
   "source": [
    "**Inspect the Dataset Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIL2eAygbi07",
    "outputId": "f9d60bca-0daa-46d7-8375-16e647e743bc"
   },
   "outputs": [],
   "source": [
    "ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZk-CH0qdov3"
   },
   "source": [
    "**Convert Hugging Face Dataset → TensorFlow**\n",
    "\n",
    "Get class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXHprVvWdPvP",
    "outputId": "f4456be7-7e44-49dc-d340-a00fb9ea75ac"
   },
   "outputs": [],
   "source": [
    "class_names = ds[\"train\"].features[\"label\"].names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(class_names)\n",
    "print(\"Classes:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke6i_cgldwW-"
   },
   "source": [
    "**Convert to TensorFlow Dataset**\n",
    "\n",
    "explicitly convert PIL → NumPy → Tensor\n",
    "\n",
    "**Note:**  \n",
    "Hugging Face datasets store images as PIL.Image\n",
    "\n",
    "TensorFlow cannot automatically convert PIL objects to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4iftNsSdyyM"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "\n",
    "def preprocess(example):\n",
    "    image = np.array(example[\"image\"])                 # PIL → NumPy\n",
    "    image = tf.convert_to_tensor(image, tf.float32)    # NumPy → Tensor\n",
    "    image = image / 255.0\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    return {\n",
    "        \"image\": image,\n",
    "        \"label\": example[\"label\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOgPPllvd9WR"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "tf_train = ds[\"train\"].with_transform(preprocess).to_tf_dataset(\n",
    "    columns=[\"image\"],\n",
    "    label_cols=\"label\",\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "tf_val = ds[\"validation\"].with_transform(preprocess).to_tf_dataset(\n",
    "    columns=[\"image\"],\n",
    "    label_cols=\"label\",\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "tf_test = ds[\"test\"].with_transform(preprocess).to_tf_dataset(\n",
    "    columns=[\"image\"],\n",
    "    label_cols=\"label\",\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDSbatKFe6-V"
   },
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBNFGRdlelhv",
    "outputId": "34c5e355-4a5d-4f2c-e7ef-07b2c1045bf5"
   },
   "outputs": [],
   "source": [
    "for images, labels in tf_train.take(1):\n",
    "    print(images.shape, images.dtype)\n",
    "    print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohooSYLqlXpR"
   },
   "source": [
    "**Architecture Overview**\n",
    "\n",
    "**1. Input Layer**\n",
    "\n",
    "Implicitly defined by input_shape=**(64,64,3)**\n",
    "\n",
    "Accepts images of size 64×64 pixels with 3 color channels (**RGB**)\n",
    "\n",
    "**Total input:** 64×64×3 = 12,288 values\n",
    "\n",
    "**2. Convolutional Block 1**\n",
    "\n",
    "\n",
    "> Conv2D(32, 3, activation=\"relu\")\n",
    "\n",
    "\n",
    "**32 filters:** Learns 32 different feature detectors\n",
    "\n",
    "**3×3 kernel size:** Each filter scans 3×3 pixel regions\n",
    "\n",
    "**ReLU activation:** Introduces non-linearity (Rectified Linear Unit)\n",
    "\n",
    "**Output shape:** After convolution, size becomes 62×62×32\n",
    "\n",
    "\n",
    "\n",
    "(64-3+1) = 62 (no padding specified)\n",
    "\n",
    "> MaxPooling2D()\n",
    "\n",
    "\n",
    "\n",
    "**Default 2×2 pooling:** Reduces spatial dimensions by half\n",
    "\n",
    "Takes maximum value from each 2×2 region\n",
    "\n",
    "**Output shape:** 31×31×32 (62/2 = 31)\n",
    "\n",
    "**3. Convolutional Block 2**\n",
    "\n",
    "\n",
    "> Conv2D(64, 3, activation=\"relu\")\n",
    "\n",
    "**64 filters:** Learns more complex features\n",
    "\n",
    "**Output shape:** 29×29×64 (31-3+1 = 29)\n",
    "\n",
    "\n",
    "> MaxPooling2D()\n",
    "\n",
    "**Output shape:** 14×14×64 (29/2 = 14.5 → rounds down to 14)\n",
    "\n",
    "**4. Convolutional Block 3**\n",
    "\n",
    "\n",
    "> Conv2D(128, 3, activation=\"relu\")\n",
    "\n",
    "\n",
    "\n",
    "**128 filters:** Learns high-level, abstract features\n",
    "\n",
    "**Output shape:** 12×12×128 (14-3+1 = 12)\n",
    "\n",
    "\n",
    "> MaxPooling2D()\n",
    "\n",
    "\n",
    "\n",
    "**Output shape:** 6×6×128 (12/2 = 6)\n",
    "\n",
    "**5. Transition to Fully Connected Layers**\n",
    "\n",
    "\n",
    "> Flatten()\n",
    "\n",
    "\n",
    "Converts 3D feature maps (6×6×128) into 1D vector\n",
    "\n",
    "**Output:** 6×6×128 = 4,608 neurons\n",
    "\n",
    "**6. Dense (Fully Connected) Layers**\n",
    "\n",
    "\n",
    "> Dense(128, activation=\"relu\")\n",
    "\n",
    "\n",
    "\n",
    "**128 neurons:** Learns combinations of features\n",
    "\n",
    "**ReLU activation:** Non-linear transformation\n",
    "\n",
    "\n",
    "> Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "\n",
    "\n",
    "**Output layer:** One neuron per class\n",
    "\n",
    "**Softmax activation:** Converts outputs to probabilities (sums to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qz2F4TlfpjT"
   },
   "source": [
    "**Build CNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "OJr4Vq02e6Fg",
    "outputId": "651fb661-9e37-41cc-fcb7-6574f3c64725"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", input_shape=(64,64,3)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.Conv2D(128, 3, activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7oHHAClfwhG"
   },
   "source": [
    "**Compile the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dKfLpU4ogPT"
   },
   "source": [
    "**1. Optimizer:**\n",
    "> optimizer=\"adam\"\n",
    "\n",
    "**Adam** (Adaptive Moment Estimation) is the most popular optimizer for deep learning:\n",
    "\n",
    "\n",
    "*   **Combines benefits:** Momentum + RMSprop\n",
    "*   **Adaptive learning rates:** Different rates for each parameter\n",
    "*   **Automatic tuning:** Adjusts learning rate during training\n",
    "\n",
    "\n",
    "**Default parameters:**\n",
    "\n",
    "\n",
    "> Learning rate: 0.001\n",
    "\n",
    "> Beta1: 0.9 (for momentum)\n",
    "\n",
    "> Beta2: 0.999 (for RMSprop)\n",
    "\n",
    "> Epsilon: 1e-7 (numerical stability)\n",
    "\n",
    "\n",
    "\n",
    "**Why Adam is preferred:**\n",
    "\n",
    "\n",
    "*   Converges faster than SGD (Stochastic Gradient Descent)\n",
    "*   Handles sparse gradients well\n",
    "*   Less sensitive to learning rate choice\n",
    "*   Good default choice for most problems\n",
    "\n",
    "**2. Loss Function:**\n",
    "> loss=\"sparse_categorical_crossentropy\"\n",
    "\n",
    "This is a **multi-class classification loss** with specific characteristics:\n",
    "\n",
    "**When to use sparse_categorical_crossentropy:**\n",
    "*  **Labels are integers:** Class indices (0, 1, 2, ...)\n",
    "\n",
    "* **Example:** If you have 3 classes, labels = [0, 2, 1, 0, ...]\n",
    "\n",
    "* **Output layer:** Must have softmax activation (outputs probabilities)\n",
    "\n",
    "**3. Metrics:**\n",
    "> metrics=[\"accuracy\"]\n",
    "\n",
    "**Accuracy** is the evaluation metric for monitoring:\n",
    "\n",
    "**Formula:**\n",
    "> (correct predictions) / (total predictions)\n",
    "\n",
    "**Computed during:** Training and validation\n",
    "\n",
    "**What you'll see in logs:**\n",
    "\n",
    "> loss: Training loss (to minimize)\n",
    "\n",
    "> accuracy: Training accuracy\n",
    "\n",
    "> val_loss: Validation loss\n",
    "\n",
    "> val_accuracy: Validation accuracy\n",
    "\n",
    "**Other common metrics you could add:**\n",
    "> metrics=[\"accuracy\", \"precision\", \"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mg_LJD_fv-J"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryZf9QIogCUY"
   },
   "source": [
    "**Train the model**\n",
    "\n",
    "**1. Training Data:**\n",
    "> tf_train\n",
    "* This is your training dataset (likely a TensorFlow Dataset object)\n",
    "* Should contain both features (images) and labels\n",
    "* Batch size is defined when creating the dataset\n",
    "\n",
    "**2. Validation Data:**\n",
    "> validation_data=tf_val\n",
    "* Separate dataset for evaluating during training\n",
    "\n",
    "* Not used for training, only for monitoring generalization\n",
    "\n",
    "* **Critical for:** Detecting overfitting, early stopping, hyperparameter tuning\n",
    "\n",
    "* Should have same structure as training data\n",
    "\n",
    "**3. Epochs:**\n",
    "> epochs=15\n",
    "\n",
    "**One epoch =** One complete pass through the entire training dataset\n",
    "\n",
    "**15 epochs =** Model sees the training data 15 times\n",
    "\n",
    "**Typical ranges:**\n",
    "\n",
    "* Simple tasks: 10-30 epochs\n",
    "\n",
    "* Complex tasks: 50-100+ epochs\n",
    "\n",
    "* Early stopping often used to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d1dJ98Of5MD",
    "outputId": "8d1e5bf5-c7df-4f2a-d26e-b387c9aeb35f"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    tf_train,\n",
    "    validation_data=tf_val,\n",
    "    epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytkoZod5r_ED"
   },
   "source": [
    "**What Happens During Training:**\n",
    "\n",
    "**For Each Epoch:**\n",
    "\n",
    "**1. Training Phase:**\n",
    "\n",
    "* Model processes training data in batches\n",
    "\n",
    "* Updates weights using backpropagation\n",
    "\n",
    "* Computes loss and accuracy\n",
    "\n",
    "**2. Validation Phase:**\n",
    "\n",
    "* Model evaluates on validation data (no weight updates)\n",
    "\n",
    "* Computes validation loss and accuracy\n",
    "\n",
    "* Shows generalization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWPAb9fWgJzY"
   },
   "source": [
    "\n",
    "\n",
    "**Evaluation Phase:**\n",
    "\n",
    "* No training occurs (weights aren't updated)\n",
    "\n",
    "* Forward pass only through the model\n",
    "\n",
    "* Computes metrics specified in model.compile()\n",
    "\n",
    "* Returns loss value + metric values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqDh_iMStNWi"
   },
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8W2bDkPgF1w",
    "outputId": "54515fd0-89e8-4ac5-ba4a-b17964515b6a"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(tf_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDEMwae1gSXN"
   },
   "source": [
    "**Sample Predictions (Visualization)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "1J5UU7u1gMn9",
    "outputId": "7104fc2b-090a-49a8-f1a9-20a6aec28f68"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for images, labels in tf_test.take(1):\n",
    "    preds = model.predict(images)\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2,5,i+1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(f\"P: {class_names[preds[i]]}\\nT: {class_names[labels[i]]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZc5ZRHzvD55"
   },
   "source": [
    "After training for 15 epochs, the model achieved a test accuracy of 0.8470. This indicates that the trained CNN is capable of classifying satellite images into their respective land-cover categories with a good level of accuracy on unseen data. While there's always room for further improvement (e.g., through more complex architectures, data augmentation, or hyperparameter tuning), this result serves as a solid foundation for further exploration in satellite image classification."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
